*   Use azure translator for each page, the source should be auto and the target "en". (check for emails before translating)
*   Check if email is valid
*   Continue searching for privacy after searching once, and don't enter to the same links after entering once (according to the button name. NOT to the link url)
*   At the start page of each domain, I look for privacy, and in its host element, search for an email.
*   Call the DB DummyB
*   Don't worry about ad popups.
*   Don't add email duplicates.
*   Run the scraper in parallel.
*   "//body//*[contains(text(),'privacy')] | //body//*[contains(text(),'Privacy')] | //body//*[contains(text(),'PRIVACY')]"
*   Limit the amount of time on each site.
*   Do the search on every opened tab.
*   Ignore Uninteractive button error.


New demands:

*   Translate each page after navigating.
*   Get all the the "a" tags, and if they contain the word privacy, so if they contain an email inside them, only get it. If not, than navigate to the "href" prop value url.
    Repeat previous steps CONFIGURABLE times (arbitrarily 2 times). insert into a duplicateCheck hashset the found emails and validate, and
    regardless to the validation result, insert into the DB with the new ScrapeResult model that's described below ! Go back in the browser history stack
    and redo everything.
*   DB insertion can be without the logger.
*   Scrape result should be extended, to have the following props: isValid, scrapeTime, urlThatContainedTheEmail and if maybe more if I think about something.
*   Provide for each scrape a configurable time.
*   Every scraping will be on a seperate task that runs on the thread pool.



//body//*[normalize-space(text()) and not(self::noscript) and not(self::style) and not(self::script)]
//body//a[contains(text(),'privacy')] | //body//a[contains(text(),'Privacy')] | //body//a[contains(text(),'PRIVACY')]